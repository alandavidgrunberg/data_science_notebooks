{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e20bd5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metrics for model w/ default .5 threshold:\n",
      "precision: 0.835820895522388\n",
      "recall: 0.6829268292682927\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression # import model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, recall_score, precision_recall_fscore_support, precision_score\n",
    "\n",
    "df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')\n",
    "df['Male'] = df['Sex'] == 'male' # create new column 'male' with Boolean 0/1 values corresponding to 'Sex' strings\n",
    "\n",
    "X = df[['Pclass', 'Male', 'Age', 'Siblings/Spouses', 'Parents/Children', 'Fare']].values\n",
    "y = df['Survived'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=5) # lock the random split with 'random_state=5'\n",
    "\n",
    "model = LogisticRegression() # instantiate model\n",
    "model.fit(X_train, y_train) # fit on training data\n",
    "y_pred = model.predict(X_test) # test on testing data\n",
    "\n",
    "## ROC curve foundations\n",
    "\n",
    "# \"Logistic Regression model doesn’t just return a prediction, but also a probability value between 0 and 1. Typically, we say if the value is >=0.5, we predict the passenger survived, and if the value is <0.5, the passenger didn’t survive. However, we could choose any threshold between 0 and 1.\"\n",
    "\n",
    "# precision = percentage of model's positive predictions that are correct\n",
    "# recall = percentage of actual positives that were predicted positively\n",
    "\n",
    "print(\"metrics for model w/ default .5 threshold:\")\n",
    "print(\"precision:\", precision_score(y_test, y_pred))\n",
    "print(\"recall:\", recall_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d34947d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix:\n",
      "[[129  11]\n",
      " [ 26  56]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# \"If we make the threshold higher, we’ll have fewer positive predictions, but our positive predictions are more likely to be correct. This means that the precision would be higher and the recall lower. If we make the threshold lower, we’ll have more positive predictions, so we’re more likely to catch all the positive cases. This means that the recall would be higher and the precision lower.\"\n",
    "\n",
    "# *\"Each choice of a threshold is a different model. ROC (Receiver operating characteristic) Curve is a graph showing all of the possible models and their performance.\"*\n",
    "\n",
    "# higher threshold = higher precision, lower recall\n",
    "# lower threshold = lower precision, higher recall\n",
    "\n",
    "# Another name for recall is sensitivity. \n",
    "# sensitivity is true positive rate. specificity is true negative rate. \n",
    "# sensitivity VS specificity demonstrate same trade-off as recall VS precision. \n",
    "\n",
    "#      A -     TN | FP\n",
    "#      c       ___|___\n",
    "#      t          |\n",
    "#      u +     FN | TP  \n",
    "#      a       \n",
    "#      l       -     +\n",
    "#          P r e d i c t e d\n",
    "\n",
    "# sensitivity = positives predicted correctly / all positive cases = TP / (TP + FN)\n",
    "# specificity = negatives predicted correctly / all negative cases = TN/ (TN + FP)\n",
    "\n",
    "print(\"confusion matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred)) # test set: 82 actual positives, 140 actual negatives\n",
    "print()\n",
    "\n",
    "#      A -    129 | 11\n",
    "#      c       ___|___\n",
    "#      t          |\n",
    "#      u +     26 | 56  \n",
    "#      a       \n",
    "#      l       -     +\n",
    "#          P r e d i c t e d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f74d884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sensitivity: 0.6829268292682927\n",
      "specificity: 0.9214285714285714\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sensitivity = TP / (TP + FN) = 56 / (56 + 26) = 56 / 82 = .6829\n",
    "# Sensitivity calculated w/ scikit-learn:\n",
    "sensitivity_score = recall_score # sensitivity is the same as recall so we assign imported 'recall_score' function to new function we name 'sensitivity_score'\n",
    "print(\"sensitivity:\", sensitivity_score(y_test, y_pred))\n",
    "\n",
    "# Specificity = TN / (TN + FP) = 129 / (129 + 11) = 129 / 140 = .9214 \n",
    "# Specificity calculated w/ scikit-learn:\n",
    "def specificity_score(y_true, y_predict): # defining new function to get specificity \n",
    "    p, r, f, s = precision_recall_fscore_support(y_true, y_predict) # imported 'precision_recall_fscore_support' function returns a list of 4 arrays of values for those 4 metrics. assigning each array to variables 'p' 'r' 'f' 's' respectively.\n",
    "    return r[0] # 'r' contains array of recall values for negative class, positive class. 'r[0]' selects first value, recall value for negative class. recall for negative class = specificity, so we have our specificity score!\n",
    "print(\"specificity:\", specificity_score(y_test, y_pred))\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e4d8213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[prob. died, prob. survived]\n",
      "[[0.46054643 0.53945357]\n",
      " [0.8881001  0.1118999 ]\n",
      " [0.13520592 0.86479408]\n",
      " [0.62497128 0.37502872]]\n",
      "\n",
      "probability of survival:\n",
      "[0.53945357 0.1118999  0.86479408 0.37502872]\n",
      "\n",
      "0.5 threshold: [1 0 1 0]\n",
      "0.75 threshold: [False False  True False]\n"
     ]
    }
   ],
   "source": [
    "# \"goal is to maximize these two values, though generally making one larger makes the other lower. depends on the situation whether we put more emphasis on sensitivity or specificity. (We graph sensitivity VS specifity curve as proxy for recall VS precision curve)\n",
    "\n",
    "## adjusting logistic regression threshold\n",
    "\n",
    "probabilities = model.predict_proba(X_test)  # 'predict_proba()' gives us probability that each data point is in 0 class (didn't surive) or 1 class (survived). notice they add up to 100%. \n",
    "print(\"[prob. died, prob. survived]\")\n",
    "print(probabilities[:4]) # show first four prediction probabilitities \n",
    "print()\n",
    "\n",
    "probabilities = model.predict_proba(X_test)[:,1] # we are only concernced with second value, probability of survival. \n",
    "print(\"probability of survival:\")\n",
    "print(probabilities[:4]) # show first four predicition probabilties \n",
    "print()\n",
    "\n",
    "y_pred = model.predict(X_test) # Models' default threshold is .5 ( > .5 = survived)\n",
    "print(\"0.5 threshold:\", y_pred[:4]) # show first four predictions\n",
    "\n",
    "y_pred = probabilities > 0.75 # changing threshold to .75 'y_pred' is array of boolean True(1)/False(0) values indicating if each datapoint met the new threshold for survival\n",
    "print(\"0.75 threshold:\", y_pred[:4]) # show first four predictions\n",
    "# \"a threshold of 0.75 means we need to be more confident in order to make a positive prediction. This results in fewer positive predictions and more negative predictions\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b7bef67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metrics for model w/ .75 threshold:\n",
      "precision: 0.9230769230769231\n",
      "recall: 0.43902439024390244\n",
      "sensitivity: 0.43902439024390244\n",
      "specificity: 0.9785714285714285\n",
      "\n",
      "confusion matrix:\n",
      "[[137   3]\n",
      " [ 46  36]]\n"
     ]
    }
   ],
   "source": [
    "print(\"metrics for model w/ .75 threshold:\")\n",
    "print(\"precision:\", precision_score(y_test, y_pred))\n",
    "print(\"recall:\", recall_score(y_test, y_pred))\n",
    "print(\"sensitivity:\", sensitivity_score(y_test, y_pred))\n",
    "print(\"specificity:\", specificity_score(y_test, y_pred))\n",
    "print()\n",
    "print(\"confusion matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred)) # notice same test set so still 82 actual positives, 140 actual negatives, but some different predictions\n",
    "\n",
    "\n",
    "## plot ROC curve (not shown here)\n",
    "\n",
    "# code and comments by github.com/alandavidgrunberg\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
