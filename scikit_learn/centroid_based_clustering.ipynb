{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5723ec24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.23606797749979\n",
      "2.23606797749979\n"
     ]
    }
   ],
   "source": [
    "### Centroid-based clustering \n",
    "\n",
    "## euclidean distance\n",
    "\n",
    "# straight line between two points. Calculated by finding the square root of sum of difference between x coordinates squared and y coordinates squared: \n",
    "\n",
    "# √(x1 - x2)^2 + (y1 - y2)^2\n",
    "\n",
    "# given points (0,1) and (2,0):\n",
    "\n",
    "# √(0 - 2)^2 + (1 - 0)^2\n",
    "# √(4 + 1)\n",
    "# √5\n",
    "\n",
    "# using numpy:\n",
    "import numpy as np\n",
    "a = np.array([0, 1])\n",
    "b = np.array([2, 0])\n",
    "print(np.sqrt(((a - b)**2).sum())) # 2.23606797749979\n",
    "print(np.sqrt(5)) # 2.23606797749979\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec1bb9ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(178, 13)\n",
      "Index(['alcohol', 'malic_acid', 'ash', 'alcalinity_of_ash', 'magnesium',\n",
      "       'total_phenols', 'flavanoids', 'nonflavanoid_phenols',\n",
      "       'proanthocyanins', 'color_intensity', 'hue',\n",
      "       'od280/od315_of_diluted_wines', 'proline'],\n",
      "      dtype='object')\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 178 entries, 0 to 177\n",
      "Data columns (total 13 columns):\n",
      " #   Column                        Non-Null Count  Dtype  \n",
      "---  ------                        --------------  -----  \n",
      " 0   alcohol                       178 non-null    float64\n",
      " 1   malic_acid                    178 non-null    float64\n",
      " 2   ash                           178 non-null    float64\n",
      " 3   alcalinity_of_ash             178 non-null    float64\n",
      " 4   magnesium                     178 non-null    float64\n",
      " 5   total_phenols                 178 non-null    float64\n",
      " 6   flavanoids                    178 non-null    float64\n",
      " 7   nonflavanoid_phenols          178 non-null    float64\n",
      " 8   proanthocyanins               178 non-null    float64\n",
      " 9   color_intensity               178 non-null    float64\n",
      " 10  hue                           178 non-null    float64\n",
      " 11  od280/od315_of_diluted_wines  178 non-null    float64\n",
      " 12  proline                       178 non-null    float64\n",
      "dtypes: float64(13)\n",
      "memory usage: 18.2 KB\n",
      "None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### clustering wines \n",
    "\n",
    "# import wine dataset as 'data'\n",
    "from sklearn.datasets import load_wine\n",
    "data = load_wine()\n",
    "\n",
    "# load 'data' pandas DataFrame 'wine'\n",
    "import pandas as pd\n",
    "wine = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "\n",
    "# check shape and columns \n",
    "print(wine.shape)\n",
    "print(wine.columns)\n",
    "print()\n",
    "# another way to check column names and data type in each column\n",
    "print(wine.info())\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98ea9d56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          alcohol  malic_acid         ash\n",
      "count  178.000000  178.000000  178.000000\n",
      "mean    13.000618    2.336348    2.366517\n",
      "std      0.811827    1.117146    0.274344\n",
      "min     11.030000    0.740000    1.360000\n",
      "25%     12.362500    1.602500    2.210000\n",
      "50%     13.050000    1.865000    2.360000\n",
      "75%     13.677500    3.082500    2.557500\n",
      "max     14.830000    5.800000    3.230000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# summary statistics for first 3 features \n",
    "print(wine.iloc[:,:3].describe()) # [all rows, columns 0 to 2]\n",
    "print()\n",
    "# can do the same thing with .loc() instead of iloc()\n",
    "# print(wine.loc[:,'alcohol':'ash'].describe())\n",
    "\n",
    "# plotting the data (scatter matrix, not shown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e87d4bc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13.00061798  2.29511236]\n",
      "[0.80954291 0.62409056]\n",
      "\n",
      "[ 7.84141790e-15 -1.95536471e-16]\n",
      "[1. 1.]\n",
      "\n",
      "[ 7.84141790e-15 -1.95536471e-16]\n",
      "[1. 1.]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Pre-processing: Standardization\n",
    "\n",
    "# \"centroid-based algorithms require one pre-processing step because k-means works better on data where each attribute is of similar scales. One way to achieve this is to standardize the data\"\n",
    "\n",
    "# Z = (X - mean) / std \n",
    "\n",
    "# X is the raw data, mean is the average of X, std is the standard deviation of X\n",
    "# Z is the scaled data such that it is centered around 0 with std of 1 \n",
    "\n",
    "X = wine[['alcohol', 'total_phenols']] # we picked these 2 features based on scatter matrix\n",
    "\n",
    "# import the scaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# instantiate the scaler\n",
    "scale = StandardScaler()\n",
    "\n",
    "# compute the mean and std to be used later for scaling\n",
    "scale.fit(X)\n",
    "\n",
    "# check calculated mean and std \n",
    "print(scale.mean_) # ['alcohol' mean, 'total_phenols' mean]\n",
    "print(scale.scale_) # ['alochol' mean, 'total phenols' mean]\n",
    "print()\n",
    "\n",
    "# transform the raw training data to scaled data\n",
    "X_scaled = scale.transform(X)\n",
    "\n",
    "# sanity check: making sure that the scaled data is centered at 0 with an std of 1 \n",
    "print(X_scaled.mean(axis=0)) # axis=0 to calculate 'alcohol' and 'total_phenoms' seperately \n",
    "print(X_scaled.std(axis=0))\n",
    "print()\n",
    "\n",
    "# we can do the same thing combining fit and transform into one step with fit_transform()\n",
    "X_scaled=scale.fit_transform(X)\n",
    "print(X_scaled.mean(axis=0))  \n",
    "print(X_scaled.std(axis=0))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "454f593c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 0 1 1 1 2 1 2 1 0 2 0 2\n",
      " 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 1 1\n",
      " 1 1 1 1 1 1 1 0 1 1 1 1 2 1 1 1 1 1 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "\n",
      "(array([0, 1, 2], dtype=int32), array([54, 59, 65]))\n",
      "\n",
      "[[ 0.05253603 -1.14020926]\n",
      " [-1.06183503  0.08414606]\n",
      " [ 0.92017418  0.87087204]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## K-means modelling: import -> instantiate -> fit -> predict\n",
    "\n",
    "# import model\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# instantiate model\n",
    "kmeans = KMeans(n_clusters=3) # 'n_clusters' pass number of clusters\n",
    "\n",
    "# \"in K-means, random initial guess for the centroids can result in bad clustering. k-means++ algorithm addresses this problem by specifying a procedure to initialize the centroids before proceeding with the standard k-means algorithm. In scikit-learn, the initialization mechanism is set to k-means++, by default\"\n",
    "\n",
    "# fit\n",
    "kmeans.fit(X_scaled)\n",
    "\n",
    "# predict\n",
    "y_pred = kmeans.predict(X_scaled)\n",
    "print(y_pred) # 1d array showing which group each wine is in\n",
    "print()\n",
    "\n",
    "# eheck how many wines in each group\n",
    "print(np.unique(y_pred, return_counts=True))\n",
    "print()\n",
    "\n",
    "# check coordinates of the 3 centroids\n",
    "print(kmeans.cluster_centers_)\n",
    "print()\n",
    "\n",
    "# (better to visualize results with scatter plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e2a8685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.00076337  0.32829793]]\n",
      "[2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## classifying a new wine \n",
    "\n",
    "# wine with 13 alcohol and 2.5 total phenols \n",
    "X_new = np.array([[13, 2.5]]) \n",
    "\n",
    "# standardize the new wine data\n",
    "X_new_scaled = scale.transform(X_new) # scale was already fit previously on model data\n",
    "print(X_new_scaled) # [[scaled 'alcohol', scaled 'total_phenoms']]\n",
    "\n",
    "# predict the cluster \n",
    "print(kmeans.predict(X_new_scaled)) # number shows which cluster it fits in\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd295501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1659.0079672511501\n",
      "1277.928488844642\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## finding optimal k: the elbow method\n",
    "\n",
    "# We can divide our dataset of n data points into any number of clusters (k) from 1 (all data points in one big cluster) to n (each data point is its own cluster). \n",
    "# So how do we know how many clusters to choose?\n",
    "\n",
    "# k-means partitions n data points into k tight sets such that the data points are closer to each other than to the data points in the other clusters. \n",
    "# The tightness can be measured as the sum of squares of the distance from data point to its nearest centroid, or inertia.\n",
    "\n",
    "# itertia = sum((x - nearest centroid)^2) \n",
    "\n",
    "#inertia for k = 2\n",
    "kmeans = KMeans(n_clusters=2)\n",
    "kmeans.fit(X_scaled)\n",
    "print(kmeans.inertia_) \n",
    "\n",
    "# inertia for k = 3\n",
    "kmeans = KMeans(n_clusters=3)\n",
    "kmeans.fit(X_scaled)\n",
    "print(kmeans.inertia_) \n",
    "print()\n",
    "\n",
    "# plot inertia for different values of k and find elbow to identify optimal k (not shown)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f0193deb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 2 0 0 0 0 0 0 0 0 0 0 0 1\n",
      " 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n",
      "\n",
      "(array([0, 1, 2], dtype=int32), array([66, 61, 51]))\n"
     ]
    }
   ],
   "source": [
    "## Modelling with all 13 features \n",
    "\n",
    "# feature selection: all of them!\n",
    "X = wine\n",
    "\n",
    "# standardize the features \n",
    "scale = StandardScaler() \n",
    "scale.fit(X)\n",
    "X_scaled = scale.transform(X)\n",
    "\n",
    "# plot inertia for different values of k and find elbow to identify optimal k (not shown)\n",
    "\n",
    "# finalize model and obtain predictions\n",
    "k_opt = 3 # the elbow plot showed us that 3 is still the optimal number of clusters \n",
    "kmeans = KMeans(k_opt)\n",
    "kmeans.fit(X_scaled)\n",
    "y_pred = kmeans.predict(X_scaled)\n",
    "print(y_pred)\n",
    "print()\n",
    "\n",
    "# eheck how many wines in each group\n",
    "print(np.unique(y_pred, return_counts=True))\n",
    "\n",
    "# compared to the predictions using only 2 features, the models produce very similiar results. Only 13 out of 178 wines were clustered differently by the two models. \n",
    "\n",
    "# which model is better? Clustering is an unsupervised learning method, which indicates we don’t know the ground truth of the labels. \n",
    "# We can't really tell which is better without access to external information. Features are often chosen by collaboration between data scientists and domain knowledge experts.\n",
    "\n",
    "# code and comments by github.com/alandavidgrunberg\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
