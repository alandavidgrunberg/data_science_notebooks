{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19a424f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression with all features\n",
      "accuracy: 0.8026915508157177\n",
      "precision: 0.7678677650001359\n",
      "recall: 0.7018155905127736\n",
      "f1 score: 0.7325793728095118\n",
      "\n",
      "Logistic Regression with Pclass, Sex & Age features\n",
      "accuracy: 0.7982035167904526\n",
      "precision: 0.7554665492957746\n",
      "recall: 0.7081643724051816\n",
      "f1 score: 0.729040996985256\n",
      "\n",
      "Logistic Regression with Fare & Age features\n",
      "accuracy: 0.6539198882752492\n",
      "precision: 0.644768115942029\n",
      "recall: 0.23062118949442892\n",
      "f1 score: 0.33924564811574187\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# comparing logisitic regression models using different features \n",
    "\n",
    "# import and prep data\n",
    "df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')\n",
    "df['Male'] = df['Sex'] == 'male'\n",
    "\n",
    "# instantiate KFold object\n",
    "kf = KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "# three different feature matrices, all with same target \n",
    "X1 = df[['Pclass', 'Male', 'Age', 'Siblings/Spouses', 'Parents/Children', 'Fare']].values\n",
    "X2 = df[['Pclass', 'Male', 'Age']].values\n",
    "X3 = df[['Fare', 'Age']].values\n",
    "y = df['Survived'].values\n",
    "\n",
    "# writing function to score the model \n",
    "def score_model(X, y, kf): # pass features, target, Kfold object\n",
    "    accuracy_scores = [] # create empty lists for each metric\n",
    "    precision_scores = [] # \"\"\n",
    "    recall_scores = [] # \"\"\n",
    "    f1_scores = [] # \"\"\n",
    "    for train_index, test_index in kf.split(X): # pass features to 'split()' method  which creates the splits, outputs a generator, for loop using training and testing indices generated for each split \n",
    "        X_train, X_test = X[train_index], X[test_index] # train/test-split for each chunk..\n",
    "        y_train, y_test = y[train_index], y[test_index] # ..using training and testing indices\n",
    "        model = LogisticRegression() # instantiate model for each chunk\n",
    "        model.fit(X_train, y_train) # fit each model\n",
    "        y_pred = model.predict(X_test) # get predictions for each model\n",
    "        accuracy_scores.append(accuracy_score(y_test, y_pred)) # passing each model's targets and predictions to get metrics..\n",
    "        precision_scores.append(precision_score(y_test, y_pred)) # ..and adding each model's metrics to respective metric lists\n",
    "        recall_scores.append(recall_score(y_test, y_pred)) # \"\n",
    "        f1_scores.append(f1_score(y_test, y_pred)) # \"\n",
    "    print(\"accuracy:\", np.mean(accuracy_scores)) # print mean of each metric list..\n",
    "    print(\"precision:\", np.mean(precision_scores)) # ..this is the cross-validated metric value\n",
    "    print(\"recall:\", np.mean(recall_scores)) # \"\n",
    "    print(\"f1 score:\", np.mean(f1_scores)) # \"\n",
    "\n",
    "print(\"Logistic Regression with all features\")\n",
    "score_model(X1, y, kf) # run 'score_model' on 1st model by passing 1st feature matrice\n",
    "print()\n",
    "print(\"Logistic Regression with Pclass, Sex & Age features\")\n",
    "score_model(X2, y, kf) # run 'score_model' on 2nd model by passing 2nd feature matrice\n",
    "print()\n",
    "print(\"Logistic Regression with Fare & Age features\")\n",
    "score_model(X3, y, kf) # run 'score_model' on 2nd model by passing 2nd feature matrice\n",
    "print()\n",
    "\n",
    "# The first two models have almost identical scores. The third model has lower scores for all four metrics. The first two are thus much better options than the third. This makes sense since the third model doesn’t have access to the sex of the passenger. Our expectation is that women are more likely to survive, so having the sex would be a very valuable predictor\n",
    "# Since the first two models have equivalent results, it makes sense to choose the simpler model, the one that uses the Pclass, Sex & Age features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a015372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n",
      "[0]\n"
     ]
    }
   ],
   "source": [
    "# Now that we’ve made a choice of a best model, we build a single final model using all of the data\n",
    "model = LogisticRegression()\n",
    "model.fit(X2, y)\n",
    "\n",
    "# Now we can make a prediction with our model\n",
    "print(model.predict([[3, False, 25]])) # passenger class 3, female, 25 years old\n",
    "print(model.predict([[3, True, 25]])) # passenger class 3, male, 25 years old\n",
    "\n",
    "# code and comments by github.com/alandavidgrunberg\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
