{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c4d6672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data dimensions: (569, 30)\n",
      "training data dimensions: (426, 30)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "cancer_data = load_breast_cancer() # instantiate breast cancer dataset as 'cancer_data'\n",
    "df = pd.DataFrame(cancer_data['data'], columns=cancer_data['feature_names']) # create DataFrame 'df' from 'cancer_data', passing 'data'  as the data, 'feature_names' as the column labels\n",
    "df['target'] = cancer_data['target'] # create new column 'target' in 'df', passing 'target' values from 'cancer_data'\n",
    "\n",
    "# features and target selection\n",
    "X = df[cancer_data.feature_names].values # select the features by passing 'feature_names' from 'cancer_data'\n",
    "y = df['target'].values # select the target\n",
    "\n",
    "print('data dimensions:', X.shape) # 569 datapoints and 30 features\n",
    "\n",
    "\n",
    "# train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=99) # set 'random_state' set to hold the split\n",
    "\n",
    "print('training data dimensions:', X_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "494ca890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction: [1 1 1 1 1]\n",
      "true value: [1 1 1 1 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# inititiate random forest model\n",
    "rf = RandomForestClassifier(random_state=93) # default settings: 100 trees, bootstrapped samples (with replacement), sub-sample size for each tree same size as original sample (426 datapoints in this case), square root of total number of features considered at each split ( âˆš30 in this case). 'random_state' holds the randomness of the bootstrapping and the feature sampling at each split\n",
    "\n",
    "# fit random forest \n",
    "rf.fit(X_train, y_train) # passing training features and targets\n",
    "\n",
    "# make prediction \n",
    "print(\"prediction:\", rf.predict(X_test[0:5])) # passing first five testing datapoints (must be passed as a [list] to make 2d) to 'rf.predict'\n",
    "print(\"true value:\", y_test[0:5]) # first five testing targets (actual values)\n",
    "print()\n",
    "# first 5 predictions correct (0 = malignant, 1 = benign)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f692c358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random forest accuracy: 0.965034965034965\n",
      "decision tree accuracy: 0.9300699300699301\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluate \n",
    "print(\"random forest accuracy:\", rf.score(X_test, y_test)) # passing all the testing datapoints and targets to get accuracy score\n",
    "\n",
    "\n",
    "# compare to decision tree model\n",
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(X_train, y_train)\n",
    "print(\"decision tree accuracy:\", dt.score(X_test, y_test))\n",
    "print()\n",
    "# we see that the accuracy score for random forest is better than for decision tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "580ceb38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best params: {'max_features': 5, 'n_estimators': 25}\n",
      "best score: 0.9648751508400633\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# tuning the random forest\n",
    "param_grid = {'n_estimators': [25, 50, 100], # number of trees\n",
    "\t\t\t  'max_features': [5, 15, 30]} # number of features considered at each split\n",
    "\t\t\t   # list of hyperparameters we want to tune and values we want to try for each one. (These are parameters for 'RandomForestClassifier()'\n",
    "gs = GridSearchCV(rf, param_grid, cv=3) # instantiate grid search object. pass random forest model, 'param_grid', 'cv=3' number of folds for kfold cross validation\n",
    "gs.fit(X, y) # fit grid search with features and targets. Grid search will build models for every possible combination of parameters. 3 'n_estimators' values * 3 'max_features' values  = 9 models. It will do 3 fold cross validation to find accuracy score for each model\n",
    "print(\"best params:\", gs.best_params_) # show winning model\n",
    "print(\"best score:\", gs.best_score_) # accuracy score for winning model\n",
    "print()\n",
    "\n",
    "# We should really test more values per parameter and do more than 3 folds for cross validation, but SoloLearn doesn't have enough computing power! \n",
    "\n",
    "# Increasing number of trees in the forest ('n_estimators') will never hurt performance: it will increase performance and then level out. However, it will add complexity which is more resource intensive. We look for the sweet spot: minimum number of trees that optimizes performance without adding unnecssary complexity. For this we use an elbow graph.\n",
    "\n",
    "# *elbow graph using matplotlib* (not shown)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "33f98f14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "most important features:\n",
      "worst concave points    0.136366\n",
      "mean concave points     0.116518\n",
      "worst perimeter         0.101182\n",
      "worst radius            0.095643\n",
      "worst area              0.090810\n",
      "dtype: float64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# feature selection via importance\n",
    "\n",
    "# The breast cancer dataset has 30 features. Which subset of features should we use to build a model? We can choose by looking at mean decrease impurity. For a tree, it can be computed how much impurity each feature decreases in a tree. And then for a forest, the impurity decrease from each feature can be averaged. Higher mean impurity decrease = more important feature.\n",
    "\n",
    "# 'feature_importances_' variable in 'RandomForestClassifier' shows feature importance scores: mean decrease impurity scores scaled down so that the sum of the scores of all features = 1.  \n",
    "\n",
    "# create a pandas Series (1d labeled arrray) of feature importances, ranked from most to least important. passing 'feature_importances_' scores from 'rf', the previously initiated and fit RandomForestClassifier. passing 'feature_names' from 'cancer_data' as the index (labels). 'sort_values' puts the scores in order, 'ascending=False' means they will be in descending order \n",
    "print('most important features:')\n",
    "print(feat_imp_rank.head(5)) # show top 5 most important features and their feature importance scores\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d55ad9f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['worst radius', 'worst texture', 'worst perimeter', 'worst area', 'worst smoothness', 'worst compactness', 'worst concavity', 'worst concave points', 'worst symmetry', 'worst fractal dimension']\n",
      "\n",
      "random forest w/ 10 features accuracy: 0.972027972027972\n"
     ]
    }
   ],
   "source": [
    "# we happen to notice that features with \"worst\" seem to have higher importances. So let's build a new model with features whose names include the word worst.\n",
    "\n",
    "feat_with_worst = [feat for feat in df.columns if 'worst' in feat] # creating a list selecting features with 'worst' in the name from 'df.columns' labels\n",
    "print(feat_with_worst) # 10 features\n",
    "print()\n",
    "\n",
    "X_with_worst = df[feat_with_worst] # new feature selection passing 'feat_with_worst' list to the DataFrame\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_with_worst, y, random_state=99) # train/test split passing selected features 'X_with_worst', same targets 'y' and 'random_state' as previous split\n",
    "\n",
    "rf.fit(X_train, y_train) # fitting new model \n",
    "print('random forest w/ 10 features accuracy:', rf.score(X_test, y_test)) # accuracy of new model \n",
    "\n",
    "# We see improved accuracy from a simpler model using only a third of the total features! This is because we removed some noise and highly correlated features. \n",
    "\n",
    "# code and comments by github.com/alandavidgrunberg\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
