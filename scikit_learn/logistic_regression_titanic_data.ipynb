{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73d247d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Survived  Pclass     Sex   Age  Siblings/Spouses  Parents/Children     Fare\n",
      "0         0       3    male  22.0                 1                 0   7.2500\n",
      "1         1       1  female  38.0                 1                 0  71.2833\n",
      "2         1       3  female  26.0                 0                 0   7.9250\n",
      "3         1       1  female  35.0                 1                 0  53.1000\n",
      "4         0       3    male  35.0                 0                 0   8.0500\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv') # 'read_csv()' takes a file in csv (comma seperated value) format and converts it to a Pandas DataFrame\n",
    "print(df.head()) # check first 5 rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b84b996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Survived  Pclass     Sex   Age  Siblings/Spouses  Parents/Children  \\\n",
      "0         0       3    male  22.0                 1                 0   \n",
      "1         1       1  female  38.0                 1                 0   \n",
      "2         1       3  female  26.0                 0                 0   \n",
      "3         1       1  female  35.0                 1                 0   \n",
      "4         0       3    male  35.0                 0                 0   \n",
      "\n",
      "      Fare   Male  \n",
      "0   7.2500   True  \n",
      "1  71.2833  False  \n",
      "2   7.9250  False  \n",
      "3  53.1000  False  \n",
      "4   8.0500   True  \n"
     ]
    }
   ],
   "source": [
    "# prepping data\n",
    "df['Male'] = df['Sex'] == 'male' # create a new column 'Male' with boolean True/False values to tell us if passenger is male or not. Booleans are easier for Python to do computations on than the strings in the 'Sex' column\n",
    "print(df.head()) # the new column is added to the end of the DataFrame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14a55144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3 True 22.0 1 0 7.25]\n",
      " [1 False 38.0 1 0 71.2833]\n",
      " [3 False 26.0 0 0 7.925]\n",
      " [1 False 35.0 1 0 53.1]\n",
      " [3 True 35.0 0 0 8.05]]\n",
      "\n",
      "[0 1 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "X = df[['Pclass', 'Male', 'Age', 'Siblings/Spouses', 'Parents/Children', 'Fare']].values\n",
    "# 2d Numpy array of features 'X' (we don't select 'Sex' column because it's non-numerical)\n",
    "y = df['Survived'].values\n",
    "# 1d Numpy array of target values 'y'\n",
    "print(X[:5]) # check first 5 datapoint features \n",
    "print()\n",
    "print(y[:5]) # check first 5 datapoint target values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3baa6681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.01615949 -0.01549065]] [-0.51037152]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## building logistic regression model \n",
    "from sklearn.linear_model import LogisticRegression # import\n",
    "X = df[['Fare', 'Age']].values # let's start with just two features first\n",
    "y = df['Survived'].values\n",
    "model = LogisticRegression() # instantiate \n",
    "model.fit(X, y) # fit \n",
    "\n",
    "print(model.coef_, model.intercept_) # check line of best fit\n",
    "print() # equation:  0 = 0.0161594x + -0.01549065y + -0.51037152\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c805f770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n",
      "[0 1 1 1 0]\n",
      "[0 1 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "## logistic regression with all features \n",
    "X = df[['Pclass', 'Male', 'Age', 'Siblings/Spouses', 'Parents/Children', 'Fare']].values\n",
    "y = df['Survived'].values\n",
    "model = LogisticRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "#predict\n",
    "print(model.predict([[3, True, 22.0, 1, 0, 7.25]])) # first passenger predicted not survived\n",
    "print(model.predict(X[:5])) # first 5 predictions (0 = not survived, 1 = survived)\n",
    "print(y[:5]) # first 5 targets (actual outcomes) we see that all 5 predictions were correct!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55b877ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "714\n",
      "0.8049605411499436\n",
      "0.8049605411499436\n"
     ]
    }
   ],
   "source": [
    "## evaluating the model\n",
    "y_pred = model.predict(X) # array of predictions\n",
    "print((y == y_pred).sum()) # sum of correct matches between target values and predictions\n",
    "print((y == y_pred).sum() / y.shape[0]) # number of correct matches / total number of passengers = accuracy score, percent accurately predicted \n",
    "print(model.score(X, y)) # same output, '.score()' uses the model to make a prediction for X and counts what percent of them match y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6d15aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[475  70]\n",
      " [103 239]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# confusion matrix\n",
    "print(confusion_matrix(y, y_pred))\n",
    "print()\n",
    "\n",
    "# predicted positive, actually positive = True Positive (TP)\n",
    "# predicted positive, actually negative = False Positive (FP)\n",
    "# predicted negative, actually negative = True Negative (TN)\n",
    "# predicted negative, actually positive = False Negative (FN)\n",
    "\n",
    "# Order of numbers in confusion matrix depends on scikit-learn conventions. This confusion matrix shows:\n",
    "\n",
    "#      A 0     TN | FP\n",
    "#      c       ___|___\n",
    "#      t          |\n",
    "#      u 1     FN | TP  \n",
    "#      a       \n",
    "#      l       0     1\n",
    "#          P r e d i c t e d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88f02fc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.8049605411499436\n",
      "precision: 0.7734627831715211\n",
      "recall: 0.6988304093567251\n",
      "f1 score: 0.7342549923195083\n"
     ]
    }
   ],
   "source": [
    "# accuracy \n",
    "print(\"accuracy:\", accuracy_score(y, y_pred))\n",
    "# TP + TN / TP + FP + TN + FN = accuracy score\n",
    "# (239 + 475) / (239+70+475+103) = 714/887 = 80.49%\n",
    "# same result as model.score(X,y)\n",
    "\n",
    "# precision\n",
    "print(\"precision:\", precision_score(y, y_pred))\n",
    "# Precision: percentage of model's positive predictions that are correct\n",
    "# (also called positive predictive value)\n",
    "\n",
    "# TP / TP + FP = precision\n",
    "# (239) / (239 + 70) = 239/309 = 77.34%\n",
    "\n",
    "# recall\n",
    "print(\"recall:\", recall_score(y, y_pred)) \n",
    "# Recall: percentage of actual positives that were predicted positively\n",
    "# (also called sensitivity)\n",
    "\n",
    "# TP / TP + FN = recall\n",
    "# (239) / (239 + 103) = 239/342 = 69.88%\n",
    "\n",
    "# Often a trade-off between precision and recall. Increasing the model's precision means making the model less sensitive, to lower the false positive rate. But a less sensitive model could also miss more actual positives, leading to a lower recall. Increasing the model's recall means making the model more sensitive, to lower the false negative rate.  But a more sensitive model could also incorrectly flag more false positives, leading to lower precision.\n",
    "\n",
    "# f1 score\n",
    "print(\"f1 score:\", f1_score(y, y_pred))\n",
    "# f1 score = harmononic mean of precision and recall, when we just want one value\n",
    "# 2 x (precision x recall)/(precision + recall) = F1 score\n",
    "# 2 x (77.34 x 69.88)/(77.34 + 69.88) = (2 x 5405.51) / (147.22) = 73.42%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d36320c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "whole dataset: (887, 6) (887,)\n",
      "training set: (665, 6) (665,)\n",
      "test set: (222, 6) (222,)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## logistic regression with data split into training and testing sets\n",
    "\n",
    "# train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y) # default 75% / 25% split\n",
    "print(\"whole dataset:\", X.shape, y.shape) # (all features, all targets)\n",
    "print(\"training set:\", X_train.shape, y_train.shape) # (training features, training targets)\n",
    "print(\"test set:\", X_test.shape, y_test.shape) # (testing features, testing targets)\n",
    "print()# notice X_train/X_test stay 2d, y_train/y_test stays 1d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7ad96c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8108108108108109\n",
      "\n",
      "accuracy: 0.8108108108108109\n",
      "precision: 0.8271604938271605\n",
      "recall: 0.7052631578947368\n",
      "f1 score: 0.7613636363636362\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# build model using training set\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# evaluate model using test set\n",
    "print(model.score(X_test, y_test))\n",
    "print()\n",
    "\n",
    "y_pred = model.predict(X_test) # array of test predictions\n",
    "print(\"accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"precision:\", precision_score(y_test, y_pred))\n",
    "print(\"recall:\", recall_score(y_test, y_pred))\n",
    "print(\"f1 score:\", f1_score(y_test, y_pred))\n",
    "print() # accuracy, precision, recall and F1 score values are similar to the values when we used the entire dataset. This is a sign our model is not overfit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b8cef701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diff split each time we run the code:\n",
      "run 1\n",
      "X_train: [[4, 4], [1, 1], [2, 2]]\n",
      "X_test: [[3, 3]]\n",
      "run 2\n",
      "X_train: [[2, 2], [1, 1], [4, 4]]\n",
      "X_test: [[3, 3]]\n",
      "\n",
      "Same split each time we run the code:\n",
      "run 1\n",
      "X_train: [[3, 3], [1, 1], [4, 4]]\n",
      "X_test: [[2, 2]]\n",
      "run 2\n",
      "X_train: [[3, 3], [1, 1], [4, 4]]\n",
      "X_test: [[2, 2]]\n"
     ]
    }
   ],
   "source": [
    "# random state\n",
    "X = [[1, 1], [2, 2], [3, 3], [4, 4]]\n",
    "y = [0, 0, 1, 1]\n",
    "# train_test_split() randomly splits the data, so we will end up with different data points in each set every time we run the code \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "print('Diff split each time we run the code:')\n",
    "print('run 1')\n",
    "print('X_train:', X_train)\n",
    "print('X_test:', X_test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "print('run 2')\n",
    "print('X_train:', X_train)\n",
    "print('X_test:', X_test)\n",
    "print()\n",
    "# 'random state=' \"locks\" the split as long as we use the same number each time\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=27)\n",
    "print('Same split each time we run the code:')\n",
    "print('run 1')\n",
    "print('X_train:', X_train)\n",
    "print('X_test:', X_test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=27)\n",
    "print('run 2')\n",
    "print('X_train:', X_train)\n",
    "print('X_test:', X_test)\n",
    "\n",
    "# the random state is also called a seed\n",
    "\n",
    "# code and comments by github.com/alandavidgrunberg\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
