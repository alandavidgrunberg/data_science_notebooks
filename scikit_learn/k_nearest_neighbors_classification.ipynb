{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc228b18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 6)\n",
      "\n",
      "   id  sepal_len  sepal_wd  petal_len  petal_wd      species\n",
      "0   0        5.1       3.5        1.4       0.2  iris-setosa\n",
      "1   1        4.9       3.0        1.4       0.2  iris-setosa\n",
      "2   2        4.7       3.2        1.3       0.2  iris-setosa\n",
      "3   3        4.6       3.1        1.5       0.2  iris-setosa\n",
      "4   4        5.0       3.6        1.4       0.2  iris-setosa\n",
      "\n",
      "   sepal_len  sepal_wd  petal_len  petal_wd      species\n",
      "0        5.1       3.5        1.4       0.2  iris-setosa\n",
      "1        4.9       3.0        1.4       0.2  iris-setosa\n",
      "2        4.7       3.2        1.3       0.2  iris-setosa\n",
      "3        4.6       3.1        1.5       0.2  iris-setosa\n",
      "4        5.0       3.6        1.4       0.2  iris-setosa\n",
      "\n",
      "        sepal_len    sepal_wd   petal_len    petal_wd\n",
      "count  150.000000  150.000000  150.000000  150.000000\n",
      "mean     5.843333    3.057333    3.758000    1.199333\n",
      "std      0.828066    0.435866    1.765298    0.762238\n",
      "min      4.300000    2.000000    1.000000    0.100000\n",
      "25%      5.100000    2.800000    1.600000    0.300000\n",
      "50%      5.800000    3.000000    4.350000    1.300000\n",
      "75%      6.400000    3.300000    5.100000    1.800000\n",
      "max      7.900000    4.400000    6.900000    2.500000\n",
      "\n",
      "iris-setosa        50\n",
      "iris-versicolor    50\n",
      "iris-virginica     50\n",
      "Name: species, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#### classification modeling with k-nearest neighbors\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# create dataframe by reading from .csv file\n",
    "iris = pd.read_csv('https://sololearn.com/uploads/files/iris.csv')\n",
    "\n",
    "print(iris.shape)\n",
    "print()\n",
    "print(iris.head())\n",
    "print()\n",
    "\n",
    "# dropping redundant 'id' column\n",
    "\n",
    "iris.drop('id', axis=1, inplace=True)\n",
    "print(iris.head())\n",
    "print()\n",
    "\n",
    "# summary statistics\n",
    "\n",
    "print(iris.describe())\n",
    "print()\n",
    "\n",
    "# check that class distribution is balanced \n",
    "\n",
    "print(iris['species'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f3fde17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iris-versicolor    35\n",
      "iris-virginica     35\n",
      "iris-setosa        35\n",
      "Name: species, dtype: int64\n",
      "\n",
      "iris-setosa        15\n",
      "iris-versicolor    15\n",
      "iris-virginica     15\n",
      "Name: species, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "## feature selection\n",
    "# petal_len and petal_wd selected because earlier we found they are most useful to seperate the species\n",
    "X = iris[['petal_len', 'petal_wd']]\n",
    "y = iris['species']\n",
    "\n",
    "## train test split\n",
    "# 70% training data, 30% testing data\n",
    "# 'stratify=' ensures distribution of species types remains similar in training and testing sets by passing 'y'\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=33, stratify=y)\n",
    "\n",
    "# verifying equal distribution of species types in training and testing sets \n",
    "print(y_train.value_counts())\n",
    "print()\n",
    "print(y_test.value_counts())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "274f7284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
      "                     weights='uniform')\n",
      "\n",
      "['iris-setosa' 'iris-versicolor' 'iris-virginica' 'iris-virginica']\n",
      "\n",
      "[[1.  0.  0. ]\n",
      " [0.  1.  0. ]\n",
      " [0.  0.  1. ]\n",
      " [0.  0.2 0.8]]\n",
      "\n",
      "14         iris-setosa\n",
      "51     iris-versicolor\n",
      "130     iris-virginica\n",
      "149     iris-virginica\n",
      "Name: species, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# \"In classifications, stratified sampling is often chosen to ensure that the train and test sets have approximately the same percentage of samples of each target class as the complete set\"\n",
    "\n",
    "# \"Remember: Import -> Instantiate -> Fit -> Predict\"\n",
    "\n",
    "## import model\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "## instantiate \n",
    "knn = KNeighborsClassifier(n_neighbors=5) #telling the model to check the 5 nearest neighbors\n",
    "\n",
    "## fit (and print to look at details of the model)\n",
    "print(knn.fit(X_train, y_train))\n",
    "print()\n",
    "\n",
    "## predict\n",
    "pred = knn.predict(X_test)\n",
    "\n",
    "# look at first 4 predictions (also called hard prediction)\n",
    "print(pred[:4])\n",
    "print()\n",
    "\n",
    "# probability prediction, outputs array showing probability of the target being each label (also called soft prediction)\n",
    "pred_prob = knn.predict_proba(X_test)\n",
    "print(pred_prob[:4])\n",
    "print()\n",
    "\n",
    "# 100% chance of first flower being setosa, \n",
    "# 100% chance of second flower being versicolor\n",
    "# 100% chance of third flower being virginica \n",
    "# 20% chance of fourth flower being versicolor, 80% chance of it being virginica. This means of the five nearest neighbours of the 4th flower in the testing set, 1 is versicolor and 4 are virginica.\n",
    "\n",
    "# first 4 actual class labels from test data set for comparision\n",
    "print(y_test[:4]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a30de8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44\n",
      "45\n",
      "\n",
      "0.9777777777777777\n",
      "0.9777777777777777\n",
      "0.9777777777777777\n"
     ]
    }
   ],
   "source": [
    "## model evaluation\n",
    "\n",
    "# most straighforward metric is accuracy, proportion of data points whose predicted labels match the observed labels.\n",
    "\n",
    "print((pred==y_test.values).sum()) # correctly predicted labels\n",
    "print(y_test.size) # total labels \n",
    "print()\n",
    "\n",
    "print((pred==y_test.values).sum()/y_test.size) # accuracy\n",
    "print(knn.score(X_test, y_test)) # .score() more easily outputs accuracy\n",
    "\n",
    "from sklearn.metrics import  accuracy_score\n",
    "print(accuracy_score(y_test, pred)) # accuracy_score() does the same thing\n",
    "# ^this accuracy score may be slightly skewed by how the data was split, as we will see later.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b93b222",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[15  0  0]\n",
      " [ 0 15  0]\n",
      " [ 0  1 14]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# confusion matrix\n",
    "\n",
    "# \"Accuracy alone can be misleading if there is an unequal number of observations in each class or if there are more than two classes in the dataset\"\n",
    "\n",
    "# \"Calculating a confusion matrix provides a better idea of what the classification is getting right and what types of errors it is making\"\n",
    "\n",
    "# \"Confusion matrix is a summary of the counts of correct and incorrect predictions, broken down by each class\"\n",
    "\n",
    "from sklearn.metrics import confusion_matrix \n",
    "print(confusion_matrix(y_test, pred)) \n",
    "print()\n",
    "# or confusion_matrix(y_test, pred, labels=['iris-setosa','iris-versicolor','iris-virginica']) to specify labels\n",
    "\n",
    "# x axis = predicted class, y axis = actual class\n",
    "# 15 setosa correctly labled, none mislabed \n",
    "# 15 versicolor correctly labled, none mislabled\n",
    "# 14 virginica correctly labled, one mislabed as versicolor \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e079c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.96666667 0.96666667 0.9        0.93333333 1.        ]\n",
      "0.9533333333333334\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### K-fold cross validation\n",
    "\n",
    "# previous train-test-split before fitting the model is a simple type of cross validation also called the holdout method. Because the split is random, model performance can be sensitive to how the data is split. \n",
    "# To overcome this we use K-fold cross validation.\n",
    "\n",
    "# the data is divided into K subsets. Then the holdout method is repeated K times, such that each time, one of the K subsets is used as the test set and the other K-1 subsets are combined to train the model. \n",
    "# Then the accuracy is averaged over K trials to provide total effectiveness of the model\n",
    "\n",
    "## import 'cross_val_score'\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "## instatiate a new knn model\n",
    "knn_cv = KNeighborsClassifier(n_neighbors=3) #telling the model to check the 3 nearest neighbors\n",
    "\n",
    "## fit (train) model with 5-fold cross validation\n",
    "## predict built-in when using 'cross_val_score()''\n",
    "cv_scores = cross_val_score(knn_cv, X, y, cv=5) # 'cv=' pass value for number of folds, usually 5 or 10 preferred \n",
    "\n",
    "## model evaluation\n",
    "\n",
    "# accuracy for each trial\n",
    "print(cv_scores)\n",
    "# average them to find the expected accuracy to report\n",
    "print(cv_scores.mean())\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64838e7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_neighbors': 4}\n",
      "0.9666666666666668\n"
     ]
    }
   ],
   "source": [
    "### tuning the hyperparameter \n",
    "\n",
    "# tuning the hyperparameter means finding the optimal k to use in a knn model\n",
    "\n",
    "# We do this using grid search. 'GridSearchCV' trains our model multiple times on a range of values & computes cross validation scores, so that we can check which value for the tested hyperparameter performed the best\n",
    "\n",
    "# 'GridsearchCV' essentially automates running 'cross_val_score' on the data using different 'n_neigbors' values and comparing the results \n",
    "\n",
    "## import 'GridSearchCV'\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "## instantiate a new knn model\n",
    "knn2 = KNeighborsClassifier() # 'n_neighbors=' not specified because this is what we're tuning\n",
    "\n",
    "# create a dict of all values we want to test for n_neighbors\n",
    "import numpy as np\n",
    "param_grid = {'n_neighbors': np.arange(2, 10)} #key is 'n_neighbors' : value is array of 2 through 9\n",
    "\n",
    "# use gridsearch to test all values in range for n_neighbors\n",
    "knn_gscv = GridSearchCV(knn2, param_grid, cv=5) #passing the model, dict of parameter(s) with values to try, and number of folds for k-fold cross validation \n",
    "\n",
    "## fit model to data\n",
    "## predict built in when using 'GridSearchCV'\n",
    "knn_gscv.fit(X, y)\n",
    "\n",
    "# check top performing 'n_neighbors' with '.best_params_'\n",
    "print(knn_gscv.best_params_) # tells us it's 4\n",
    "# check accuracy when 'n_neighbors' is 4 with '.best_score_'\n",
    "print(knn_gscv.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "709ae2ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9733333333333334\n"
     ]
    }
   ],
   "source": [
    "### now we are ready to build the final model\n",
    "\n",
    "## import - KNeighborsClassifier already imported \n",
    "\n",
    "## instantiate \n",
    "knn_final = KNeighborsClassifier(n_neighbors=knn_gscv.best_params_['n_neighbors'])\n",
    "\n",
    "## fit\n",
    "knn_final.fit(X, y)\n",
    "\n",
    "## predict\n",
    "y_pred = knn_final.predict(X)\n",
    "\n",
    "## model evaluation\n",
    "# accuracy\n",
    "print(knn_final.score(X, y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c47320fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2)\n",
      "\n",
      "['iris-versicolor']\n",
      "['iris-versicolor']\n",
      "\n",
      "['iris-versicolor' 'iris-virginica' 'iris-setosa']\n",
      "[[0.   1.   0.  ]\n",
      " [0.   0.25 0.75]\n",
      " [1.   0.   0.  ]]\n"
     ]
    }
   ],
   "source": [
    "## label prediction with new data\n",
    "\n",
    "# unknown iris with petal length of  3.76 cm and petal width of 1.20 cm \n",
    "new_data = np.array([3.76, 1.20])\n",
    "\n",
    "# must reshape new data from 1d to 2d array because that's what the model was trained on and expects as input\n",
    "new_data = new_data.reshape(1, -1)\n",
    "print(new_data.shape)\n",
    "print()\n",
    "\n",
    "# predict\n",
    "print(knn_final.predict(new_data))\n",
    "\n",
    "# instead of reshaping we can just input a 2d list, same result \n",
    "print(knn_final.predict([[3.76, 1.2]]))\n",
    "print()\n",
    "\n",
    "\n",
    "## probability prediction with new data\n",
    "\n",
    "# three irises, different lengths, all have the same petal width of 1.2 cm\n",
    "new_data = np.array([[3.76, 1.2], [5.25, 1.2], [1.58, 1.2]])\n",
    "\n",
    "# predict \n",
    "print(knn_final.predict(new_data))\n",
    "\n",
    "# probability predict\n",
    "print(knn_final.predict_proba(new_data))\n",
    "# 100% chance of first flower being versicolor\n",
    "# 25% chance of second flower being versicolor, 75% chance of it being virginica\n",
    "# 100% chance of third flower being setosa \n",
    "\n",
    "# code and comments by github.com/alandavidgrunberg\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
