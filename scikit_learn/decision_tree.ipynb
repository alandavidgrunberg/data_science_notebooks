{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef1c40d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n",
      "\n",
      "Decision Tree - single train/test split\n",
      "  accuracy: 0.7657657657657657\n",
      "  precision: 0.7073170731707317\n",
      "  recall: 0.6744186046511628\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "import numpy as np\n",
    "\n",
    "# code for decision tree model similar to logistic regression model. Makes it to build and compare models for the same dataset\n",
    "\n",
    "df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')\n",
    "df['Male'] = df['Sex'] == 'male'\n",
    "\n",
    "# features and target selection\n",
    "X = df[['Pclass', 'Male', 'Age', 'Siblings/Spouses', 'Parents/Children', 'Fare']].values\n",
    "y = df['Survived'].values\n",
    "\n",
    "\n",
    "## decision tree using train/test split\n",
    "\n",
    "# train/test-split \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y) # split features and targets into train/test sets\n",
    "\n",
    "# instantiate model\n",
    "model = DecisionTreeClassifier() \n",
    "\n",
    "# fit\n",
    "model.fit(X_train, y_train) # passing training features and targets\n",
    "\n",
    "# predict\n",
    "print(model.predict([[3, True, 22, 1, 0, 7.25]])) \n",
    "print()\n",
    "# 3rd class, male passenger, age 22, with 1 sibiling/spouse, 0 parents/children, 7.25 fare\n",
    "# predicted to not survive\n",
    "\n",
    "# evaluate \n",
    "y_pred = model.predict(X_test) # passing testing features to get predictions\n",
    "print(\"Decision Tree - single train/test split\")\n",
    "print(\"  accuracy:\", accuracy_score(y_test, y_pred)) #  passing testing targets and predictions to get metrics \n",
    "print(\"  precision:\", precision_score(y_test, y_pred)) # \"\n",
    "print(\"  recall:\", recall_score(y_test, y_pred)) # \"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad9d151a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree - k-fold cross validated\n",
      "  accuracy: 0.7587062781692376\n",
      "  precision: 0.687061714459847\n",
      "  recall: 0.7022506633151794\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## decision tree using k-fold cross validation for more accurate metrics \n",
    "\n",
    "# instantiate kfold object\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=23) # 5 chunks, random_state to lock the split\n",
    "\n",
    "accuracy_scores = [] # create empty lists for metrics\n",
    "precision_scores = [] # \"\n",
    "recall_scores = [] # \"\n",
    "for train_index, test_index in kf.split(X): # pass features to 'kf.split()' method  which creates the splits, outputs a generator. for loop using training and testing indices generated for each fold\n",
    "    X_train, X_test = X[train_index], X[test_index] # train/test split for each fold using training and testing indices \n",
    "    y_train, y_test = y[train_index], y[test_index] # \"\n",
    "    dt = DecisionTreeClassifier(random_state=9) # instantiate model for each fold, internal random_state for decision tree\n",
    "    dt.fit(X_train, y_train) # fit each model, passing training features and targets \n",
    "    y_pred = dt.predict(X_test) # get predictions for each model, passing testing features \n",
    "    accuracy_scores.append(accuracy_score(y_test, y_pred)) # passing each model's testing targets and predictions to get metrics and adding each model's metrics to respective metric list\n",
    "    precision_scores.append(precision_score(y_test, y_pred)) # \"\n",
    "    recall_scores.append(recall_score(y_test, y_pred)) # \"\n",
    "print(\"Decision Tree - k-fold cross validated\")\n",
    "print(\"  accuracy:\", np.mean(accuracy_scores)) # print mean of each metric list, this is the cross-validated metric value\n",
    "print(\"  precision:\", np.mean(precision_scores)) # \"\n",
    "print(\"  recall:\", np.mean(recall_scores)) # \"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "875b5566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree\n",
      "  accuracy: 0.7587062781692376\n",
      "  precision: 0.687061714459847\n",
      "  recall: 0.7022506633151794\n",
      "\n",
      "*versus*\n",
      "\n",
      "Logistic Regression\n",
      "  accuracy: 0.7993461562876912\n",
      "  precision: 0.7648070841239722\n",
      "  recall: 0.7003502304147465\n"
     ]
    }
   ],
   "source": [
    "## comparing decision tree to logistic regression, using k-fold cross validation\n",
    "\n",
    "dt_accuracy_scores = [] # create empty lists for decision tree metrics\n",
    "dt_precision_scores = [] # \"\n",
    "dt_recall_scores = [] # \"\n",
    "lr_accuracy_scores = [] # create empty lists for logistic regression metrics\n",
    "lr_precision_scores = [] # \"\n",
    "lr_recall_scores = [] # \"\n",
    "for train_index, test_index in kf.split(X): # pass features to 'kf.split()' method  which creates the splits, outputs a generator. for loop using training and testing indices generated for each fold\n",
    "    X_train, X_test = X[train_index], X[test_index] # train/test split for each fold using training and testing indices \n",
    "    y_train, y_test = y[train_index], y[test_index] # \"\n",
    "    dt = DecisionTreeClassifier(random_state=9) # instantiate decision tree model for each fold, same internal random_state as above\n",
    "    dt.fit(X_train, y_train) # fit each model, passing training features and targets \n",
    "    dt_y_pred = dt.predict(X_test) # get predictions for each model, passing testing features \n",
    "    dt_accuracy_scores.append(accuracy_score(y_test, dt_y_pred)) # passing each model's testing targets and predictions to get metrics and adding each model's metrics to respective metric list\n",
    "    dt_precision_scores.append(precision_score(y_test, dt_y_pred)) # \"\n",
    "    dt_recall_scores.append(recall_score(y_test, dt_y_pred)) # \"\n",
    "    lr = LogisticRegression() # instantiate logistic regression model for each fold\n",
    "    lr.fit(X_train, y_train) # fit each model, passing training features and targets \n",
    "    lr_y_pred = lr.predict(X_test) # get predictions for each model, passing testing features\n",
    "    lr_accuracy_scores.append(accuracy_score(y_test, lr_y_pred)) # passing each model's testing targets and predictions to get metrics and adding each model's metrics to respective metric list\n",
    "    lr_precision_scores.append(precision_score(y_test, lr_y_pred)) # \"\n",
    "    lr_recall_scores.append(recall_score(y_test, lr_y_pred)) # \"\n",
    "print(\"Decision Tree\")\n",
    "print(\"  accuracy:\", np.mean(dt_accuracy_scores)) # print mean of each metric list for decision tree, this is the cross-validated metric value\n",
    "print(\"  precision:\", np.mean(dt_precision_scores)) # \"\n",
    "print(\"  recall:\", np.mean(dt_recall_scores)) # \"\n",
    "print()\n",
    "print(\"*versus*\")\n",
    "print()\n",
    "print(\"Logistic Regression\")\n",
    "print(\"  accuracy:\", np.mean(lr_accuracy_scores)) # \"\n",
    "print(\"  precision:\", np.mean(lr_precision_scores)) # \"\n",
    "print(\"  recall:\", np.mean(lr_recall_scores)) # \"\n",
    "\n",
    "# we see that the logistic regression model performs better, though we still may want to use a decision tree model for its interpretability \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1a22726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree\n",
      "  accuracy: 0.7587062781692376\n",
      "\n",
      "*versus*\n",
      "\n",
      "Logistic Regression\n",
      "  accuracy: 0.7993461562876912\n"
     ]
    }
   ],
   "source": [
    "## comparing decision tree to logistic regression, using 'cross_val_score' shortcut\n",
    "\n",
    "dt = DecisionTreeClassifier(random_state=9) # instantiate decision tree model, same internal 'random_state' as above\n",
    "lr = LogisticRegression() # instantiate logistic regression model\n",
    "\n",
    "dt_accuracy_scores = cross_val_score(dt, X, y, cv=kf) # pass model, features, targets, number of folds to 'cross_val_score' Passing 'kf' instead of a number. 'kf' is the KFold object that was initiated earlier with (n_splits=5, shuffle=True, random_state=23) to match the random state from before\n",
    "lr_accuracy_scores = cross_val_score(lr, X , y, cv=kf)  # 'cross_val_score' will automatically create a train/test split for each fold, instantiate a model for each fold, fit each model, test each model, and output each model's accuracy score\n",
    "print(\"Decision Tree\")\n",
    "print(\"  accuracy:\", np.mean(dt_accuracy_scores)) # mean of the 5 accuracy scores\n",
    "print()\n",
    "print(\"*versus*\")\n",
    "print()\n",
    "print(\"Logistic Regression\")\n",
    "print(\"  accuracy:\", np.mean(lr_accuracy_scores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4458519c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree - gini\n",
      "accuracy: 0.7587062781692376\n",
      "precision: 0.687061714459847\n",
      "recall: 0.7022506633151794 \n",
      "\n",
      "*versus*\n",
      "Decision Tree - entropy\n",
      "accuracy: 0.772259252205929\n",
      "precision: 0.7088325436802231\n",
      "recall: 0.7069822184983476 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## comparing decision tree using gini impurity to decision tree using entropy\n",
    "\n",
    "# quality of the split (information gain) can be measured via gini impurity or entropy. Default is gini impurity. Can be adjusted with 'critereon=' parameter, passing 'gini' or 'entropy'\n",
    "\n",
    "for criteria in ['gini', 'entropy']: # pass first 'gini' then 'entropy' to 'criteria'\n",
    "    print(f\"Decision Tree - {criteria}\") #f-string print string w/ 'criteria'\n",
    "    accuracy = [] # create empty lists for metrics (first for 'gini' then for 'entropy')\n",
    "    precision = [] # \"\n",
    "    recall = [] # \"\n",
    "    # following 'for' loop done twice, first for 'gini' then for 'entropy'\n",
    "    for train_index, test_index in kf.split(X): # pass features to 'kf.split()' method which creates the splits, outputs a generator. for loop using training and testing indices generated for each fold\n",
    "        X_train, X_test = X[train_index], X[test_index] # train/test split for each fold using training and testing indices \n",
    "        y_train, y_test = y[train_index], y[test_index] # \"\n",
    "        dt = DecisionTreeClassifier(random_state=9, criterion=criteria) # instantiate decision tree model for each fold, same random_state as above, 'criterion=' pass 'criteria' (first 'gini' then entropy')\n",
    "        dt.fit(X_train, y_train) # fit each model, passing training features and targets \n",
    "        y_pred = dt.predict(X_test) # get predictions for each model, passing testing features \n",
    "        accuracy.append(accuracy_score(y_test, y_pred)) # passing each model's testing targets and predictions to get metrics and adding each model's metrics to respective metric list\n",
    "        precision.append(precision_score(y_test, y_pred)) # \"\n",
    "        recall.append(recall_score(y_test, y_pred)) # \"\n",
    "    print(\"accuracy:\", np.mean(accuracy)) # print mean of each metric list, this is the cross-validated metric value\n",
    "    print(\"precision:\", np.mean(precision)) # \"\n",
    "    print(\"recall:\", np.mean(recall), '\\n') # \" n\\ for line break\n",
    "    if criteria=='gini':\n",
    "        print(\"*versus*\") # prints only after decision tree with gini's metrics\n",
    "\n",
    "\n",
    "# decision tree using entropy performs better but only very slightly. Rare to find a dataset where the choice would make a big difference\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "556e6497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best params: {'max_depth': 15, 'max_leaf_nodes': 35, 'min_samples_leaf': 1}\n",
      "best score: 0.7746390877915417\n"
     ]
    }
   ],
   "source": [
    "## pre-pruning the decision tree to reduce overfitting. comparing potential options using GridSearchCV\n",
    "\n",
    "# We will compare different values for 'max_depth' (max n splits each data point) 'min_samples_leaf' (min n of samples threshold to stop splitting) and 'max_leaf_nodes' (max total n of leaf nodes allowed on tree)\n",
    "\n",
    "# Instead of looping over different options like we've been doing above, we will use scikitlearn's built-in GridSearchCV, which automates this process for us and tells us the best parameters \n",
    "\n",
    "param_grid = {\n",
    "    'max_depth': [5, 15, 25],\n",
    "    'min_samples_leaf': [1, 3],\n",
    "    'max_leaf_nodes': [10, 20, 35, 50]} # list of hyperparameters we want to tune and values we want to try for each one. (These are parameters for 'DecisionTreeClassifier()')\n",
    "\n",
    "dt = DecisionTreeClassifier() # instantiate decision tree model\n",
    "\n",
    "gs = GridSearchCV(dt, param_grid, scoring='f1', cv=5) # instantiate grid search object. pass model, 'param_grid', 'scoring=' use f1 score as metric to determine best parameters, 'cv=5' number of folds for kfold cross validation\n",
    "\n",
    "gs.fit(X, y) # fit grid search with features and targets. Grid search will build models for every possible combination of parameters. 3 'max_depth' values * 2 'min_samples_leaf' values * 4 'max_leaf_nodes' = 24 models. It will do 5 fold cross validation to find f1 score for each model\n",
    "\n",
    "print(\"best params:\", gs.best_params_) # show which model won\n",
    "print(\"best score:\", gs.best_score_) # f1 score for winning model\n",
    "\n",
    "#  Might get slightly different results on each run  depending on the randomness of how the points are distributed among the folds. Generally if we have multiple models with comparable performance, we’d choose the simpler model\n",
    "\n",
    "# code and comments by github.com/alandavidgrunberg\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
